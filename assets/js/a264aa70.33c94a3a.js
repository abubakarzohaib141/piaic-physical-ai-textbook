"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5915],{5680:(e,n,a)=>{a.d(n,{xA:()=>c,yg:()=>d});var r=a(6540);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,r)}return a}function s(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach(function(n){i(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function t(e,n){if(null==e)return{};var a,r,i=function(e,n){if(null==e)return{};var a,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=r.createContext({}),m=function(e){var n=r.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):s(s({},n),e)),a},c=function(e){var n=m(e.components);return r.createElement(l.Provider,{value:n},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},p=r.forwardRef(function(e,n){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=t(e,["components","mdxType","originalType","parentName"]),g=m(a),p=i,d=g["".concat(l,".").concat(p)]||g[p]||u[p]||o;return a?r.createElement(d,s(s({ref:n},c),{},{components:a})):r.createElement(d,s({ref:n},c))});function d(e,n){var a=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=a.length,s=new Array(o);s[0]=p;var t={};for(var l in n)hasOwnProperty.call(n,l)&&(t[l]=n[l]);t.originalType=e,t[g]="string"==typeof e?e:i,s[1]=t;for(var m=2;m<o;m++)s[m]=a[m];return r.createElement.apply(null,s)}return r.createElement.apply(null,a)}p.displayName="MDXCreateElement"},6632:(e,n,a)=>{a.r(n),a.d(n,{contentTitle:()=>s,default:()=>g,frontMatter:()=>o,metadata:()=>t,toc:()=>l});var r=a(8168),i=(a(6540),a(5680));const o={},s="Gazebo Sensors - Simulating Robot Perception",t={unversionedId:"physical-ai/module2-gazebo/sensors",id:"physical-ai/module2-gazebo/sensors",isDocsHomePage:!1,title:"Gazebo Sensors - Simulating Robot Perception",description:"Overview",source:"@site/docs/physical-ai/module2-gazebo/sensors.md",sourceDirName:"physical-ai/module2-gazebo",slug:"/physical-ai/module2-gazebo/sensors",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module2-gazebo/sensors",editUrl:"https://github.com/abubakarzohaib141/piaic-physical-ai-textbook/edit/main/docs/docs/physical-ai/module2-gazebo/sensors.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 2: The Digital Twin - Gazebo Simulation",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module2-gazebo/overview"},next:{title:"Navigation with Nav2",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module3-isaac/navigation"}},l=[{value:"Overview",id:"overview",children:[]},{value:"Camera Sensors",id:"camera-sensors",children:[{value:"RGB Camera",id:"rgb-camera",children:[]},{value:"Subscribing to Camera Data",id:"subscribing-to-camera-data",children:[]},{value:"Depth Camera (Intel RealSense Simulation)",id:"depth-camera-intel-realsense-simulation",children:[]}]},{value:"LiDAR Sensors",id:"lidar-sensors",children:[{value:"2D LiDAR (Planar Scan)",id:"2d-lidar-planar-scan",children:[]},{value:"Processing LiDAR Data",id:"processing-lidar-data",children:[]},{value:"3D LiDAR (Velodyne-style)",id:"3d-lidar-velodyne-style",children:[]}]},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",children:[{value:"IMU Sensor Definition",id:"imu-sensor-definition",children:[]},{value:"Processing IMU Data",id:"processing-imu-data",children:[]}]},{value:"Sensor Fusion Example",id:"sensor-fusion-example",children:[]},{value:"Visualizing Sensor Data in RViz2",id:"visualizing-sensor-data-in-rviz2",children:[]},{value:"Performance Considerations",id:"performance-considerations",children:[]},{value:"Next Steps",id:"next-steps",children:[]},{value:"Resources",id:"resources",children:[]}],m={toc:l},c="wrapper";function g({components:e,...n}){return(0,i.yg)(c,(0,r.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"gazebo-sensors---simulating-robot-perception"},"Gazebo Sensors - Simulating Robot Perception"),(0,i.yg)("h2",{id:"overview"},"Overview"),(0,i.yg)("p",null,"For a humanoid robot to navigate and interact with the world, it needs ",(0,i.yg)("strong",{parentName:"p"},"sensors"),". Gazebo can simulate the three most critical sensor types:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Cameras")," (RGB, Depth, Stereo) - Vision"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"LiDAR")," - 3D environment mapping"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"IMU")," (Inertial Measurement Unit) - Balance and orientation")),(0,i.yg)("h2",{id:"camera-sensors"},"Camera Sensors"),(0,i.yg)("h3",{id:"rgb-camera"},"RGB Camera"),(0,i.yg)("p",null,"Adding a camera to your URDF:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'\x3c!-- Camera link --\x3e\n<link name="camera_link">\n  <visual>\n    <geometry>\n      <box size="0.02 0.05 0.02"/>\n    </geometry>\n    <material name="black"/>\n  </visual>\n  <collision>\n    <geometry>\n      <box size="0.02 0.05 0.02"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="0.1"/>\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n  </inertial>\n</link>\n\n\x3c!-- Camera joint (mounted on head) --\x3e\n<joint name="camera_joint" type="fixed">\n  <parent link="head"/>\n  <child link="camera_link"/>\n  <origin xyz="0.1 0 0" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Gazebo camera plugin --\x3e\n<gazebo reference="camera_link">\n  <sensor name="camera" type="camera">\n    <update_rate>30</update_rate>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.02</near>\n        <far>300</far>\n      </clip>\n    </camera>\n    \n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <argument>image_raw:=camera/image_raw</argument>\n        <argument>camera_info:=camera/camera_info</argument>\n      </ros>\n      <camera_name>head_camera</camera_name>\n      <frame_name>camera_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n')),(0,i.yg)("h3",{id:"subscribing-to-camera-data"},"Subscribing to Camera Data"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass CameraProcessor(Node):\n    def __init__(self):\n        super().__init__('camera_processor')\n        self.bridge = CvBridge()\n        \n        self.image_sub = self.create_subscription(\n            Image,\n            '/humanoid/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.info_sub = self.create_subscription(\n            CameraInfo,\n            '/humanoid/camera/camera_info',\n            self.info_callback,\n            10\n        )\n        \n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        \n        # Process image (object detection, etc.)\n        processed = self.detect_objects(cv_image)\n        \n        # Display (optional)\n        cv2.imshow('Camera View', processed)\n        cv2.waitKey(1)\n        \n    def info_callback(self, msg):\n        # Store camera calibration\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n")),(0,i.yg)("h3",{id:"depth-camera-intel-realsense-simulation"},"Depth Camera (Intel RealSense Simulation)"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'<gazebo reference="camera_link">\n  <sensor name="depth_camera" type="depth">\n    <update_rate>20</update_rate>\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n    </camera>\n    \n    <plugin name="depth_camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n      </ros>\n      <camera_name>depth_camera</camera_name>\n      <frame_name>camera_link</frame_name>\n      <min_depth>0.1</min_depth>\n      <max_depth>10.0</max_depth>\n    </plugin>\n  </sensor>\n</gazebo>\n')),(0,i.yg)("p",null,"This publishes:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/humanoid/depth_camera/image_raw")," - RGB image"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/humanoid/depth_camera/depth/image_raw")," - Depth image"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"/humanoid/depth_camera/points")," - Point cloud")),(0,i.yg)("h2",{id:"lidar-sensors"},"LiDAR Sensors"),(0,i.yg)("p",null,"LiDAR provides 3D point clouds for navigation and obstacle avoidance."),(0,i.yg)("h3",{id:"2d-lidar-planar-scan"},"2D LiDAR (Planar Scan)"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'<gazebo reference="lidar_link">\n  <sensor name="lidar" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.10</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </ray>\n    \n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <argument>~/out:=scan</argument>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>lidar_link</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n')),(0,i.yg)("h3",{id:"processing-lidar-data"},"Processing LiDAR Data"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from sensor_msgs.msg import LaserScan\n\nclass LidarProcessor(Node):\n    def __init__(self):\n        super().__init__('lidar_processor')\n        \n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            '/humanoid/scan',\n            self.scan_callback,\n            10\n        )\n        \n    def scan_callback(self, msg):\n        # msg.ranges contains distance measurements\n        # msg.angle_min, msg.angle_max, msg.angle_increment\n        \n        # Find closest obstacle\n        min_distance = min(msg.ranges)\n        min_index = msg.ranges.index(min_distance)\n        angle = msg.angle_min + min_index * msg.angle_increment\n        \n        self.get_logger().info(\n            f'Closest obstacle: {min_distance:.2f}m at {np.degrees(angle):.1f}\xb0'\n        )\n        \n        # Obstacle avoidance logic\n        if min_distance < 0.5:\n            self.get_logger().warn('OBSTACLE TOO CLOSE!')\n")),(0,i.yg)("h3",{id:"3d-lidar-velodyne-style"},"3D LiDAR (Velodyne-style)"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'<gazebo reference="lidar_link">\n  <sensor name="velodyne" type="gpu_ray">\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>1800</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n        <vertical>\n          <samples>16</samples>\n          <resolution>1</resolution>\n          <min_angle>-0.2618</min_angle>  \x3c!-- -15 degrees --\x3e\n          <max_angle>0.2618</max_angle>   \x3c!-- +15 degrees --\x3e\n        </vertical>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>100</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    \n    <plugin name="gazebo_ros_velodyne_laser_controller" filename="libgazebo_ros_velodyne_laser.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <argument>~/out:=velodyne_points</argument>\n      </ros>\n      <frame_name>lidar_link</frame_name>\n      <min_range>0.9</min_range>\n      <max_range>100.0</max_range>\n    </plugin>\n  </sensor>\n</gazebo>\n')),(0,i.yg)("h2",{id:"imu-inertial-measurement-unit"},"IMU (Inertial Measurement Unit)"),(0,i.yg)("p",null,"Critical for humanoid balance and orientation tracking."),(0,i.yg)("h3",{id:"imu-sensor-definition"},"IMU Sensor Definition"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-xml"},'<gazebo reference="imu_link">\n  <sensor name="imu" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    \n    <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <argument>~/out:=imu</argument>\n      </ros>\n      <frame_name>imu_link</frame_name>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n    </plugin>\n  </sensor>\n</gazebo>\n')),(0,i.yg)("h3",{id:"processing-imu-data"},"Processing IMU Data"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from sensor_msgs.msg import Imu\nimport numpy as np\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n        \n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid/imu',\n            self.imu_callback,\n            10\n        )\n        \n    def imu_callback(self, msg):\n        # Orientation (quaternion)\n        quat = [\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        ]\n        \n        # Convert to Euler angles (roll, pitch, yaw)\n        roll, pitch, yaw = self.quaternion_to_euler(quat)\n        \n        # Angular velocity\n        angular_vel = [\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ]\n        \n        # Linear acceleration\n        linear_accel = [\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ]\n        \n        # Check if robot is falling\n        if abs(roll) > 0.5 or abs(pitch) > 0.5:\n            self.get_logger().warn('ROBOT UNSTABLE!')\n            \n    def quaternion_to_euler(self, quat):\n        # Convert quaternion to Euler angles\n        # [implementation here]\n        return roll, pitch, yaw\n")),(0,i.yg)("h2",{id:"sensor-fusion-example"},"Sensor Fusion Example"),(0,i.yg)("p",null,"Combining camera + depth + IMU for robust perception:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"from sensor_msgs.msg import Image, Imu\nfrom message_filters import Subscriber, ApproximateTimeSynchronizer\n\nclass SensorFusion(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n        \n        # Synchronized subscribers\n        image_sub = Subscriber(self, Image, '/humanoid/camera/image_raw')\n        depth_sub = Subscriber(self, Image, '/humanoid/camera/depth/image_raw')\n        imu_sub = Subscriber(self, Imu, '/humanoid/imu')\n        \n        self.sync = ApproximateTimeSynchronizer(\n            [image_sub, depth_sub, imu_sub],\n            queue_size=10,\n            slop=0.1\n        )\n        self.sync.registerCallback(self.sensor_callback)\n        \n    def sensor_callback(self, image_msg, depth_msg, imu_msg):\n        # All three sensors are now synchronized\n        self.get_logger().info('Processing fused sensor data')\n        \n        # Use IMU to compensate for robot motion in vision\n        # Use depth to get 3D positions of detected objects\n        # Combine for robust environmental understanding\n")),(0,i.yg)("h2",{id:"visualizing-sensor-data-in-rviz2"},"Visualizing Sensor Data in RViz2"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"rviz2\n")),(0,i.yg)("p",null,"In RViz2:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Add \u2192 Camera")," - View camera feed"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Add \u2192 LaserScan")," - Visualize 2D LiDAR"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Add \u2192 PointCloud2")," - View 3D LiDAR or depth point cloud"),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Add \u2192 Axes")," - Show IMU orientation")),(0,i.yg)("h2",{id:"performance-considerations"},"Performance Considerations"),(0,i.yg)("blockquote",null,(0,i.yg)("p",{parentName:"blockquote"},"[!TIP]","\n",(0,i.yg)("strong",{parentName:"p"},"Reduce sensor update rates")," if simulation is slow:"),(0,i.yg)("ul",{parentName:"blockquote"},(0,i.yg)("li",{parentName:"ul"},"Cameras: 10-30 Hz is often sufficient"),(0,i.yg)("li",{parentName:"ul"},"LiDAR: 5-10 Hz for navigation"),(0,i.yg)("li",{parentName:"ul"},"IMU: 100 Hz for balance control"))),(0,i.yg)("blockquote",null,(0,i.yg)("p",{parentName:"blockquote"},"[!WARNING]","\n",(0,i.yg)("strong",{parentName:"p"},"GPU acceleration required")," for multiple cameras or high-resolution LiDAR. Use ",(0,i.yg)("inlineCode",{parentName:"p"},"gpu_ray")," sensor type when available.")),(0,i.yg)("h2",{id:"next-steps"},"Next Steps"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},(0,i.yg)("a",{parentName:"strong",href:"/piaic-physical-ai-textbook/docs/physical-ai/module3-isaac/overview"},"Module 3: NVIDIA Isaac Overview"))," - Hardware-accelerated perception"),(0,i.yg)("li",{parentName:"ul"},"Apply these sensors in navigation and manipulation tasks")),(0,i.yg)("h2",{id:"resources"},"Resources"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"https://gazebosim.org/docs/harmonic/sensors"},"Gazebo Sensors")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"https://github.com/ros2/common_interfaces/tree/rolling/sensor_msgs"},"ROS 2 Sensor Messages")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Writing-A-Tf2-Broadcaster-Cpp.html"},"cv_bridge Tutorial"))))}g.isMDXComponent=!0}}]);