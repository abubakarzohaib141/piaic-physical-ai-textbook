"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[2589],{4133:(e,n,a)=>{a.r(n),a.d(n,{contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var t=a(8168),o=(a(6540),a(5680));const i={},r="Module 4: Vision-Language-Action (VLA)",l={unversionedId:"physical-ai/module4-vla/overview",id:"physical-ai/module4-vla/overview",isDocsHomePage:!1,title:"Module 4: Vision-Language-Action (VLA)",description:"Overview",source:"@site/docs/physical-ai/module4-vla/overview.md",sourceDirName:"physical-ai/module4-vla",slug:"/physical-ai/module4-vla/overview",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module4-vla/overview",editUrl:"https://github.com/abubakarzohaib141/piaic-physical-ai-textbook/edit/main/docs/docs/physical-ai/module4-vla/overview.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 3: The AI-Robot Brain - NVIDIA Isaac",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module3-isaac/overview"},next:{title:"Voice-to-Action Pipeline - Complete Implementation",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module4-vla/voice-to-action"}},s=[{value:"Overview",id:"overview",children:[]},{value:"The VLA Revolution",id:"the-vla-revolution",children:[]},{value:"Architecture",id:"architecture",children:[]},{value:"Components",id:"components",children:[{value:"1. Voice Interface (OpenAI Whisper)",id:"1-voice-interface-openai-whisper",children:[]},{value:"2. LLM-Based Task Planner",id:"2-llm-based-task-planner",children:[]},{value:"3. Vision-Language Integration",id:"3-vision-language-integration",children:[]},{value:"4. Action Execution",id:"4-action-execution",children:[]}]},{value:"End-to-End Example: &quot;Bring Me a Cup&quot;",id:"end-to-end-example-bring-me-a-cup",children:[]},{value:"State-of-the-Art VLA Models",id:"state-of-the-art-vla-models",children:[{value:"RT-2 (Robotic Transformer 2)",id:"rt-2-robotic-transformer-2",children:[]},{value:"OpenVLA",id:"openvla",children:[]},{value:"PaLM-E",id:"palm-e",children:[]}]},{value:"Challenges",id:"challenges",children:[]},{value:"Next Steps",id:"next-steps",children:[]},{value:"Resources",id:"resources",children:[]}],c={toc:s},p="wrapper";function m({components:e,...n}){return(0,o.yg)(p,(0,t.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"module-4-vision-language-action-vla"},"Module 4: Vision-Language-Action (VLA)"),(0,o.yg)("h2",{id:"overview"},"Overview"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Vision-Language-Action (VLA)")," models represent the convergence of:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Vision"),": Understanding the world through cameras"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Language"),": Natural communication with humans"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Action"),": Executing physical tasks")),(0,o.yg)("p",null,"This enables robots to understand commands like ",(0,o.yg)("em",{parentName:"p"},'"Pick up the red cup on the table"')," and translate them into motor commands."),(0,o.yg)("h2",{id:"the-vla-revolution"},"The VLA Revolution"),(0,o.yg)("p",null,"Traditional robotics required:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"\u274c Hand-coded state machines for every task"),(0,o.yg)("li",{parentName:"ul"},"\u274c Explicit object detection and manipulation pipelines"),(0,o.yg)("li",{parentName:"ul"},"\u274c Complex behavior trees")),(0,o.yg)("p",null,"VLA models enable:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"\u2705 ",(0,o.yg)("strong",{parentName:"li"},"Natural language commands")," \u2192 robot actions"),(0,o.yg)("li",{parentName:"ul"},"\u2705 ",(0,o.yg)("strong",{parentName:"li"},"Foundation models")," trained on internet-scale data"),(0,o.yg)("li",{parentName:"ul"},"\u2705 ",(0,o.yg)("strong",{parentName:"li"},"Few-shot learning")," - adapt to new tasks quickly"),(0,o.yg)("li",{parentName:"ul"},"\u2705 ",(0,o.yg)("strong",{parentName:"li"},"Multimodal understanding")," - vision + language together")),(0,o.yg)("h2",{id:"architecture"},"Architecture"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-mermaid"},"graph LR\n    A[Voice Input] --\x3e B[OpenAI Whisper]\n    B --\x3e C[Text Command]\n    C --\x3e D[LLM Planner]\n    E[Camera] --\x3e F[Vision Encoder]\n    F --\x3e D\n    D --\x3e G[Task Decomposition]\n    G --\x3e H[ROS 2 Actions]\n    H --\x3e I[Robot Execution]\n    I --\x3e J[Visual Feedback]\n    J --\x3e D\n")),(0,o.yg)("h2",{id:"components"},"Components"),(0,o.yg)("h3",{id:"1-voice-interface-openai-whisper"},"1. Voice Interface (OpenAI Whisper)"),(0,o.yg)("p",null,"Convert speech to text:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import whisper\nfrom sensor_msgs.msg import Audio\nimport numpy as np\n\nclass VoiceInterface(Node):\n    def __init__(self):\n        super().__init__(\'voice_interface\')\n        \n        # Load Whisper model\n        self.model = whisper.load_model("base")  # or "medium", "large"\n        \n        # Subscribe to microphone\n        self.audio_sub = self.create_subscription(\n            Audio,\n            \'/audio/input\',\n            self.audio_callback,\n            10\n        )\n        \n        # Publish transcribed commands\n        self.command_pub = self.create_publisher(\n            String,\n            \'/voice_commands\',\n            10\n        )\n        \n        self.audio_buffer = []\n        \n    def audio_callback(self, msg):\n        self.audio_buffer.extend(msg.data)\n        \n        # Process every 3 seconds of audio\n        if len(self.audio_buffer) >= 48000 * 3:  # 48kHz * 3s\n            self.process_audio()\n            self.audio_buffer = []\n            \n    def process_audio(self):\n        # Convert to numpy array\n        audio_np = np.array(self.audio_buffer, dtype=np.float32)\n        \n        # Transcribe with Whisper\n        result = self.model.transcribe(audio_np)\n        text = result["text"]\n        \n        self.get_logger().info(f\'Heard: "{text}"\')\n        \n        # Publish command\n        msg = String()\n        msg.data = text\n        self.command_pub.publish(msg)\n')),(0,o.yg)("h3",{id:"2-llm-based-task-planner"},"2. LLM-Based Task Planner"),(0,o.yg)("p",null,"Use GPT-4 to decompose tasks:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from openai import OpenAI\nfrom std_msgs.msg import String\n\nclass TaskPlanner(Node):\n    def __init__(self):\n        super().__init__(\'task_planner\')\n        \n        # Initialize OpenAI client\n        self.client = OpenAI(api_key="your-api-key")\n        \n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'/voice_commands\',\n            self.command_callback,\n            10\n        )\n        \n        # Publish action sequence\n        self.action_pub = self.create_publisher(\n            ActionSequence,  # Custom message\n            \'/robot/actions\',\n            10\n        )\n        \n    def command_callback(self, msg):\n        command = msg.data\n        \n        # Get robot capabilities\n        capabilities = self.get_robot_capabilities()\n        \n        # Query LLM for plan\n        plan = self.generate_plan(command, capabilities)\n        \n        # Execute plan\n        self.execute_plan(plan)\n        \n    def generate_plan(self, command, capabilities):\n        prompt = f"""\nYou are controlling a humanoid robot with the following capabilities:\n{capabilities}\n\nThe user said: "{command}"\n\nBreak this down into a sequence of atomic actions the robot can perform.\nRespond in JSON format with an array of actions:\n[\n    {{"action": "navigate_to", "params": {{"location": "kitchen"}}}},\n    {{"action": "detect_object", "params": {{"object": "cup", "color": "red"}}}},\n    {{"action": "grasp_object", "params": {{"object_id": "detected_cup"}}}}\n]\n"""\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": "You are a robot task planner."},\n                {"role": "user", "content": prompt}\n            ]\n        )\n        \n        plan_json = response.choices[0].message.content\n        return json.loads(plan_json)\n        \n    def get_robot_capabilities(self):\n        return """\n- navigate_to(location): Move to a named location\n- detect_object(object, color): Find an object by description\n- grasp_object(object_id): Pick up a detected object\n- place_object(location): Put down held object\n- speak(text): Say something to the user\n"""\n        \n    def execute_plan(self, plan):\n        for step in plan:\n            self.get_logger().info(f\'Executing: {step["action"]}\')\n            \n            if step["action"] == "navigate_to":\n                self.navigate_to(step["params"]["location"])\n            elif step["action"] == "detect_object":\n                self.detect_object(step["params"])\n            elif step["action"] == "grasp_object":\n                self.grasp_object(step["params"])\n            # ... etc\n')),(0,o.yg)("h3",{id:"3-vision-language-integration"},"3. Vision-Language Integration"),(0,o.yg)("p",null,"Combine camera input with LLM reasoning:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport base64\nimport cv2\n\nclass VisionLanguage(Node):\n    def __init__(self):\n        super().__init__(\'vision_language\')\n        \n        self.bridge = CvBridge()\n        self.client = OpenAI()\n        \n        self.image_sub = self.create_subscription(\n            Image,\n            \'/humanoid/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        self.current_image = None\n        \n    def image_callback(self, msg):\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, \'rgb8\')\n        \n    def ask_about_scene(self, question):\n        if self.current_image is None:\n            return "No camera feed available"\n            \n        # Encode image as base64\n        _, buffer = cv2.imencode(\'.jpg\', self.current_image)\n        image_base64 = base64.b64encode(buffer).decode(\'utf-8\')\n        \n        # Query GPT-4 Vision\n        response = self.client.chat.completions.create(\n            model="gpt-4-vision-preview",\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {"type": "text", "text": question},\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{image_base64}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            max_tokens=300\n        )\n        \n        return response.choices[0].message.content\n')),(0,o.yg)("h3",{id:"4-action-execution"},"4. Action Execution"),(0,o.yg)("p",null,"Convert high-level actions to ROS 2:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"from nav2_msgs.action import NavigateToPose\nfrom manipulation_msgs.action import GraspObject\n\nclass ActionExecutor(Node):\n    def __init__(self):\n        super().__init__('action_executor')\n        \n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.grasp_client = ActionClient(self, GraspObject, 'grasp_object')\n        \n    def navigate_to(self, location_name):\n        # Look up location in semantic map\n        pose = self.get_location_pose(location_name)\n        \n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = pose\n        \n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.nav_response_callback)\n        \n    def grasp_object(self, object_description):\n        # Detect object\n        object_pose = self.detect_and_localize(object_description)\n        \n        # Plan grasp\n        goal_msg = GraspObject.Goal()\n        goal_msg.target_pose = object_pose\n        goal_msg.approach_vector = [0, 0, -1]  # From above\n        \n        future = self.grasp_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.grasp_response_callback)\n")),(0,o.yg)("h2",{id:"end-to-end-example-bring-me-a-cup"},'End-to-End Example: "Bring Me a Cup"'),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class VLARobot(Node):\n    def __init__(self):\n        super().__init__(\'vla_robot\')\n        \n        self.whisper_model = whisper.load_model("base")\n        self.openai_client = OpenAI()\n        \n        # Microphone input\n        self.setup_audio_pipeline()\n        \n        # Camera input\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        \n        self.current_image = None\n        \n    def run(self):\n        while rclpy.ok():\n            # 1. Listen for command\n            command = self.listen_for_command()\n            self.get_logger().info(f\'Command: {command}\')\n            \n            # 2. Understand scene\n            scene_description = self.describe_scene()\n            self.get_logger().info(f\'Scene: {scene_description}\')\n            \n            # 3. Plan task\n            plan = self.plan_task(command, scene_description)\n            self.get_logger().info(f\'Plan: {plan}\')\n            \n            # 4. Execute\n            self.execute_plan(plan)\n            \n            # 5. Confirm\n            self.speak("Task completed!")\n            \n    def listen_for_command(self):\n        # Record 5 seconds of audio\n        audio = self.record_audio(duration=5.0)\n        \n        # Transcribe\n        result = self.whisper_model.transcribe(audio)\n        return result["text"]\n        \n    def describe_scene(self):\n        response = self.openai_client.chat.completions.create(\n            model="gpt-4-vision-preview",\n            messages=[{\n                "role": "user",\n                "content": [\n                    {"type": "text", "text": "Describe this scene. What objects do you see?"},\n                    {"type": "image_url", "image_url": {"url": self.encode_image(self.current_image)}}\n                ]\n            }]\n        )\n        return response.choices[0].message.content\n        \n    def plan_task(self, command, scene):\n        prompt = f"""\nScene: {scene}\nCommand: {command}\n\nGenerate a step-by-step plan to accomplish this command.\n"""\n        response = self.openai_client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": "You are a robot task planner."},\n                {"role": "user", "content": prompt}\n            ]\n        )\n        return response.choices[0].message.content\n')),(0,o.yg)("h2",{id:"state-of-the-art-vla-models"},"State-of-the-Art VLA Models"),(0,o.yg)("h3",{id:"rt-2-robotic-transformer-2"},"RT-2 (Robotic Transformer 2)"),(0,o.yg)("p",null,"Google's VLA model:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Trained on internet images + robot demonstrations"),(0,o.yg)("li",{parentName:"ul"},"Directly outputs robot actions"),(0,o.yg)("li",{parentName:"ul"},"Generalizes to new objects")),(0,o.yg)("h3",{id:"openvla"},"OpenVLA"),(0,o.yg)("p",null,"Open-source alternative:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Install OpenVLA\npip install openvla\n\nfrom openvla import OpenVLAModel\n\nmodel = OpenVLAModel.from_pretrained("openvla/openvla-7b")\n\n# Get action from image + text\naction = model.predict(\n    image=camera_image,\n    instruction="pick up the red block"\n)\n\n# action = [x, y, z, roll, pitch, yaw, gripper]\n')),(0,o.yg)("h3",{id:"palm-e"},"PaLM-E"),(0,o.yg)("p",null,"Embodied multimodal model:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"562B parameters"),(0,o.yg)("li",{parentName:"ul"},"Processes images, text, sensor data"),(0,o.yg)("li",{parentName:"ul"},"Reasons about physical world")),(0,o.yg)("h2",{id:"challenges"},"Challenges"),(0,o.yg)("blockquote",null,(0,o.yg)("p",{parentName:"blockquote"},"[!WARNING]","\n",(0,o.yg)("strong",{parentName:"p"},"Hallucination"),": LLMs may suggest impossible actions. Always validate against robot capabilities.")),(0,o.yg)("blockquote",null,(0,o.yg)("p",{parentName:"blockquote"},"[!IMPORTANT]","\n",(0,o.yg)("strong",{parentName:"p"},"Safety"),": Implement hard-coded safety constraints before LLM-generated actions:"),(0,o.yg)("ul",{parentName:"blockquote"},(0,o.yg)("li",{parentName:"ul"},"Joint limits"),(0,o.yg)("li",{parentName:"ul"},"Workspace boundaries"),(0,o.yg)("li",{parentName:"ul"},"Collision avoidance"))),(0,o.yg)("blockquote",null,(0,o.yg)("p",{parentName:"blockquote"},"[!TIP]","\n",(0,o.yg)("strong",{parentName:"p"},"Prompt Engineering"),": Spend time crafting system prompts that accurately describe robot capabilities and constraints.")),(0,o.yg)("h2",{id:"next-steps"},"Next Steps"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},(0,o.yg)("a",{parentName:"strong",href:"/piaic-physical-ai-textbook/docs/physical-ai/module4-vla/voice-to-action"},"Voice-to-Action Pipeline"))," - Complete implementation"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Practice"),": Build a simple VLA demo"),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Advanced"),": Fine-tune VLA models on your robot")),(0,o.yg)("h2",{id:"resources"},"Resources"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"https://robotics-transformer2.github.io/"},"RT-2 Paper")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"https://github.com/openvla/openvla"},"OpenVLA")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"https://github.com/openai/whisper"},"OpenAI Whisper")),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("a",{parentName:"li",href:"https://platform.openai.com/docs/guides/vision"},"GPT-4 Vision API"))))}m.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>p,yg:()=>d});var t=a(6540);function o(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function r(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){o(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function l(e,n){if(null==e)return{};var a,t,o=function(e,n){if(null==e)return{};var a,t,o={},i=Object.keys(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||(o[a]=e[a]);return o}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)a=i[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=t.createContext({}),c=function(e){var n=t.useContext(s),a=n;return e&&(a="function"==typeof e?e(n):r(r({},n),e)),a},p=function(e){var n=c(e.components);return t.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},g=t.forwardRef(function(e,n){var a=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=c(a),g=o,d=m["".concat(s,".").concat(g)]||m[g]||u[g]||i;return a?t.createElement(d,r(r({ref:n},p),{},{components:a})):t.createElement(d,r({ref:n},p))});function d(e,n){var a=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:o,r[1]=l;for(var c=2;c<i;c++)r[c]=a[c];return t.createElement.apply(null,r)}return t.createElement.apply(null,a)}g.displayName="MDXCreateElement"}}]);