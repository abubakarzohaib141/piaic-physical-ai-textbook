"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7366],{2985:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var a=t(8168),o=(t(6540),t(5680));const r={},i="Voice-to-Action Pipeline - Complete Implementation",s={unversionedId:"physical-ai/module4-vla/voice-to-action",id:"physical-ai/module4-vla/voice-to-action",isDocsHomePage:!1,title:"Voice-to-Action Pipeline - Complete Implementation",description:"Overview",source:"@site/docs/physical-ai/module4-vla/voice-to-action.md",sourceDirName:"physical-ai/module4-vla",slug:"/physical-ai/module4-vla/voice-to-action",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module4-vla/voice-to-action",editUrl:"https://github.com/abubakarzohaib141/piaic-physical-ai-textbook/edit/main/docs/docs/physical-ai/module4-vla/voice-to-action.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Module 4: Vision-Language-Action (VLA)",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module4-vla/overview"},next:{title:"13-Week Course Outline",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/week-by-week/outline"}},l=[{value:"Overview",id:"overview",children:[]},{value:"System Architecture",id:"system-architecture",children:[]},{value:"Step 1: Audio Capture Node",id:"step-1-audio-capture-node",children:[]},{value:"Step 2: Whisper Transcription Node",id:"step-2-whisper-transcription-node",children:[]},{value:"Step 3: GPT-4 Task Planner Node",id:"step-3-gpt-4-task-planner-node",children:[]},{value:"Step 4: Action Executor Node",id:"step-4-action-executor-node",children:[]},{value:"Step 5: Launch File",id:"step-5-launch-file",children:[]},{value:"Testing the System",id:"testing-the-system",children:[]},{value:"Improvements",id:"improvements",children:[{value:"Add Confirmation",id:"add-confirmation",children:[]},{value:"Add Context Memory",id:"add-context-memory",children:[]}]},{value:"Security Considerations",id:"security-considerations",children:[]}],c={toc:l},p="wrapper";function u({components:e,...n}){return(0,o.yg)(p,(0,a.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"voice-to-action-pipeline---complete-implementation"},"Voice-to-Action Pipeline - Complete Implementation"),(0,o.yg)("h2",{id:"overview"},"Overview"),(0,o.yg)("p",null,"This guide walks through building a complete ",(0,o.yg)("strong",{parentName:"p"},"Voice-to-Action (V2A)")," system that:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Captures voice commands via microphone"),(0,o.yg)("li",{parentName:"ol"},"Transcribes speech using OpenAI Whisper"),(0,o.yg)("li",{parentName:"ol"},"Plans tasks using GPT-4"),(0,o.yg)("li",{parentName:"ol"},"Executes actions via ROS 2"),(0,o.yg)("li",{parentName:"ol"},"Provides visual and audio feedback")),(0,o.yg)("h2",{id:"system-architecture"},"System Architecture"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-mermaid"},'sequenceDiagram\n    participant User\n    participant Mic as Microphone Node\n    participant Whisper as Whisper Node\n    participant GPT as GPT-4 Planner\n    participant Exec as Action Executor\n    participant Robot as Humanoid Robot\n    \n    User->>Mic: "Pick up the cup"\n    Mic->>Whisper: Audio buffer\n    Whisper->>GPT: "Pick up the cup"\n    GPT->>Exec: [navigate, detect, grasp]\n    Exec->>Robot: ROS 2 Actions\n    Robot->>User: Visual confirmation\n    Exec->>User: "Task complete!"\n')),(0,o.yg)("h2",{id:"step-1-audio-capture-node"},"Step 1: Audio Capture Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"# audio_capture_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import ByteMultiArray, Bool\nimport pyaudio\nimport numpy as np\nimport threading\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture')\n        \n        # Parameters\n        self.declare_parameter('sample_rate', 16000)\n        self.declare_parameter('chunk_size', 1024)\n        self.declare_parameter('channels', 1)\n        \n        self.sample_rate = self.get_parameter('sample_rate').value\n        self.chunk_size = self.get_parameter('chunk_size').value\n        self.channels = self.get_parameter('channels').value\n        \n        # Publishers\n        self.audio_pub = self.create_publisher(\n            ByteMultiArray,\n            '/audio/raw',\n            10\n        )\n        \n        self.is_speaking_pub = self.create_publisher(\n            Bool,\n            '/audio/is_speaking',\n            10\n        )\n        \n        # PyAudio setup\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size,\n            stream_callback=self.audio_callback\n        )\n        \n        self.stream.start_stream()\n        self.get_logger().info('Audio capture started')\n        \n    def audio_callback(self, in_data, frame_count, time_info, status):\n        # Convert to numpy array\n        audio_data = np.frombuffer(in_data, dtype=np.int16)\n        \n        # Detect speech (simple energy-based VAD)\n        energy = np.sqrt(np.mean(audio_data**2))\n        is_speaking = energy > 500  # Threshold\n        \n        # Publish audio data\n        msg = ByteMultiArray()\n        msg.data = in_data\n        self.audio_pub.publish(msg)\n        \n        # Publish speech detection\n        speaking_msg = Bool()\n        speaking_msg.data = is_speaking\n        self.is_speaking_pub.publish(speaking_msg)\n        \n        return (in_data, pyaudio.paContinue)\n        \n    def destroy_node(self):\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n\n\ndef main():\n    rclpy.init()\n    node = AudioCaptureNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n")),(0,o.yg)("h2",{id:"step-2-whisper-transcription-node"},"Step 2: Whisper Transcription Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"# whisper_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import ByteMultiArray, Bool, String\nimport whisper\nimport numpy as np\nfrom collections import deque\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_transcription')\n        \n        # Load model\n        self.declare_parameter('model_size', 'base')  # tiny, base, small, medium, large\n        model_size = self.get_parameter('model_size').value\n        \n        self.get_logger().info(f'Loading Whisper model: {model_size}')\n        self.model = whisper.load_model(model_size)\n        self.get_logger().info('Whisper model loaded')\n        \n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            ByteMultiArray,\n            '/audio/raw',\n            self.audio_callback,\n            10\n        )\n        \n        self.speaking_sub = self.create_subscription(\n            Bool,\n            '/audio/is_speaking',\n            self.speaking_callback,\n            10\n        )\n        \n        # Publisher\n        self.text_pub = self.create_publisher(\n            String,\n            '/voice/transcription',\n            10\n        )\n        \n        # State\n        self.audio_buffer = deque(maxlen=160000)  # 10 seconds at 16kHz\n        self.is_speaking = False\n        self.was_speaking = False\n        \n    def audio_callback(self, msg):\n        # Add to buffer\n        audio_chunk = np.frombuffer(msg.data, dtype=np.int16)\n        self.audio_buffer.extend(audio_chunk)\n        \n    def speaking_callback(self, msg):\n        self.was_speaking = self.is_speaking\n        self.is_speaking = msg.data\n        \n        # Detect end of speech\n        if self.was_speaking and not self.is_speaking:\n            self.process_speech()\n            \n    def process_speech(self):\n        if len(self.audio_buffer) < 16000:  # Minimum 1 second\n            return\n            \n        # Convert to float32\n        audio_np = np.array(self.audio_buffer, dtype=np.float32) / 32768.0\n        \n        # Transcribe\n        self.get_logger().info('Transcribing...')\n        result = self.model.transcribe(\n            audio_np,\n            language='en',\n            task='transcribe'\n        )\n        \n        text = result[\"text\"].strip()\n        self.get_logger().info(f'Transcription: \"{text}\"')\n        \n        # Publish\n        msg = String()\n        msg.data = text\n        self.text_pub.publish(msg)\n        \n        # Clear buffer\n        self.audio_buffer.clear()\n\n\ndef main():\n    rclpy.init()\n    node = WhisperNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n")),(0,o.yg)("h2",{id:"step-3-gpt-4-task-planner-node"},"Step 3: GPT-4 Task Planner Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# gpt4_planner_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom robot_interfaces.msg import ActionSequence, RobotAction\nfrom openai import OpenAI\nimport json\n\nclass GPT4PlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'gpt4_planner\')\n        \n        # Get API key from parameter\n        self.declare_parameter(\'openai_api_key\', \'\')\n        api_key = self.get_parameter(\'openai_api_key\').value\n        \n        self.client = OpenAI(api_key=api_key)\n        \n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            \'/voice/transcription\',\n            self.command_callback,\n            10\n        )\n        \n        # Publishers\n        self.plan_pub = self.create_publisher(\n            ActionSequence,\n            \'/robot/action_plan\',\n            10\n        )\n        \n        self.feedback_pub = self.create_publisher(\n            String,\n            \'/robot/feedback\',\n            10\n        )\n        \n    def command_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f\'Planning for: "{command}"\')\n        \n        # Generate plan\n        plan = self.generate_plan(command)\n        \n        # Publish plan\n        if plan:\n            self.publish_plan(plan)\n        else:\n            self.publish_feedback("I didn\'t understand that command.")\n            \n    def generate_plan(self, command):\n        system_prompt = """\nYou are a humanoid robot task planner. You can perform these actions:\n\n1. navigate_to(location: str) - Move to a location ("kitchen", "living room", etc.)\n2. search_for_object(object_name: str) - Look for an object\n3. grasp_object(object_name: str) - Pick up an object\n4. place_object(location: str) - Put down held object\n5. say(text: str) - Speak to the user\n\nGiven a user command, generate a JSON array of actions to accomplish it.\n\nExample:\nUser: "Bring me a cup from the kitchen"\nResponse: [\n  {"action": "navigate_to", "params": {"location": "kitchen"}},\n  {"action": "search_for_object", "params": {"object": "cup"}},\n  {"action": "grasp_object", "params": {"object": "cup"}},\n  {"action": "navigate_to", "params": {"location": "user"}},\n  {"action": "place_object", "params": {"location": "table"}},\n  {"action": "say", "params": {"text": "Here is your cup"}}\n]\n\nOnly respond with the JSON array, nothing else.\n"""\n        \n        try:\n            response = self.client.chat.completions.create(\n                model="gpt-4-turbo-preview",\n                messages=[\n                    {"role": "system", "content": system_prompt},\n                    {"role": "user", "content": command}\n                ],\n                temperature=0.3\n            )\n            \n            plan_text = response.choices[0].message.content\n            plan = json.loads(plan_text)\n            \n            self.get_logger().info(f\'Generated plan with {len(plan)} steps\')\n            return plan\n            \n        except Exception as e:\n            self.get_logger().error(f\'Planning failed: {e}\')\n            return None\n            \n    def publish_plan(self, plan):\n        msg = ActionSequence()\n        \n        for step in plan:\n            action = RobotAction()\n            action.action_type = step["action"]\n            action.parameters = json.dumps(step.get("params", {}))\n            msg.actions.append(action)\n            \n        self.plan_pub.publish(msg)\n        \n    def publish_feedback(self, text):\n        msg = String()\n        msg.data = text\n        self.feedback_pub.publish(msg)\n')),(0,o.yg)("h2",{id:"step-4-action-executor-node"},"Step 4: Action Executor Node"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"# action_executor_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom robot_interfaces.msg import ActionSequence\nfrom nav2_msgs.action import NavigateToPose\nfrom manipulation_msgs.action import GraspObject\nimport json\n\nclass ActionExecutorNode(Node):\n    def __init__(self):\n        super().__init__('action_executor')\n        \n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        \n        # Subscribers\n        self.plan_sub = self.create_subscription(\n            ActionSequence,\n            '/robot/action_plan',\n            self.plan_callback,\n            10\n        )\n        \n        self.current_plan = None\n        self.current_step = 0\n        \n    def plan_callback(self, msg):\n        self.current_plan = msg.actions\n        self.current_step = 0\n        self.execute_next_step()\n        \n    def execute_next_step(self):\n        if self.current_step >= len(self.current_plan):\n            self.get_logger().info('Plan completed!')\n            self.current_plan = None\n            return\n            \n        action = self.current_plan[self.current_step]\n        self.get_logger().info(f'Executing: {action.action_type}')\n        \n        if action.action_type == 'navigate_to':\n            self.execute_navigate(json.loads(action.parameters))\n        elif action.action_type == 'grasp_object':\n            self.execute_grasp(json.loads(action.parameters))\n        elif action.action_type == 'say':\n            self.execute_say(json.loads(action.parameters))\n        else:\n            self.get_logger().warn(f'Unknown action: {action.action_type}')\n            self.current_step += 1\n            self.execute_next_step()\n            \n    def execute_navigate(self, params):\n        location = params['location']\n        \n        # Look up location pose (simplified)\n        pose = self.get_location_pose(location)\n        \n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = pose\n        \n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.nav_done_callback)\n        \n    def nav_done_callback(self, future):\n        self.current_step += 1\n        self.execute_next_step()\n        \n    def execute_say(self, params):\n        text = params['text']\n        # Use TTS system\n        self.get_logger().info(f'Robot says: \"{text}\"')\n        self.current_step += 1\n        self.execute_next_step()\n")),(0,o.yg)("h2",{id:"step-5-launch-file"},"Step 5: Launch File"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"# v2a_launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Audio capture\n        Node(\n            package='voice_to_action',\n            executable='audio_capture_node',\n            name='audio_capture',\n            parameters=[{\n                'sample_rate': 16000,\n                'chunk_size': 1024\n            }]\n        ),\n        \n        # Whisper transcription\n        Node(\n            package='voice_to_action',\n            executable='whisper_node',\n            name='whisper',\n            parameters=[{\n                'model_size': 'base'\n            }]\n        ),\n        \n        # GPT-4 planner\n        Node(\n            package='voice_to_action',\n            executable='gpt4_planner_node',\n            name='gpt4_planner',\n            parameters=[{\n                'openai_api_key': 'your-api-key-here'\n            }]\n        ),\n        \n        # Action executor\n        Node(\n            package='voice_to_action',\n            executable='action_executor_node',\n            name='action_executor'\n        )\n    ])\n")),(0,o.yg)("h2",{id:"testing-the-system"},"Testing the System"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},"# Build workspace\ncd ~/ros2_ws\ncolcon build --packages-select voice_to_action\n\n# Source\nsource install/setup.bash\n\n# Launch\nros2 launch voice_to_action v2a_launch.py\n\n# Monitor transcriptions\nros2 topic echo /voice/transcription\n\n# Monitor plans\nros2 topic echo /robot/action_plan\n")),(0,o.yg)("h2",{id:"improvements"},"Improvements"),(0,o.yg)("h3",{id:"add-confirmation"},"Add Confirmation"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'def command_callback(self, msg):\n    command = msg.data\n    \n    # Confirm understanding\n    confirmation = f"I will {command.lower()}. Is that correct?"\n    self.publish_feedback(confirmation)\n    \n    # Wait for "yes" or "no"\n    # Then generate plan\n')),(0,o.yg)("h3",{id:"add-context-memory"},"Add Context Memory"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'class GPT4PlannerNode(Node):\n    def __init__(self):\n        # ...\n        self.conversation_history = []\n        \n    def generate_plan(self, command):\n        self.conversation_history.append({\n            "role": "user",\n            "content": command\n        })\n        \n        response = self.client.chat.completions.create(\n            model="gpt-4",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                *self.conversation_history\n            ]\n        )\n        \n        # Store response\n        self.conversation_history.append({\n            "role": "assistant",\n            "content": response.choices[0].message.content\n        })\n')),(0,o.yg)("h2",{id:"security-considerations"},"Security Considerations"),(0,o.yg)("blockquote",null,(0,o.yg)("p",{parentName:"blockquote"},"[!CAUTION]","\n",(0,o.yg)("strong",{parentName:"p"},"API Key Security"),": Never hardcode API keys. Use environment variables or ROS 2 parameters with proper file permissions.")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},'# Store API key securely\nexport OPENAI_API_KEY="sk-..."\n\n# Pass to node\nros2 run voice_to_action gpt4_planner_node --ros-args -p openai_api_key:=$OPENAI_API_KEY\n')),(0,o.yg)("blockquote",null,(0,o.yg)("p",{parentName:"blockquote"},"[!WARNING]","\n",(0,o.yg)("strong",{parentName:"p"},"Command Validation"),": Always validate LLM outputs before execution:"),(0,o.yg)("pre",{parentName:"blockquote"},(0,o.yg)("code",{parentName:"pre",className:"language-python"},"# Whitelist of allowed actions\nALLOWED_ACTIONS = ['navigate_to', 'grasp_object', 'place_object', 'say']\n"))),(0,o.yg)("p",null,"for action in plan:\nif action","['action']",' not in ALLOWED_ACTIONS:\nraise ValueError(f"Invalid action: {action',"['action']",'}")'),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"\n## Next Steps\n\n- **[Course Outline](/piaic-physical-ai-textbook/docs/physical-ai/week-by-week/outline)** - Review 13-week schedule\n- **Capstone Project**: Build autonomous humanoid with V2A\n- **Advanced**: Add error recovery and multi-turn conversations\n\n## Resources\n\n- [OpenAI Whisper](https://github.com/openai/whisper)\n- [OpenAI API Docs](https://platform.openai.com/docs/api-reference)\n- [PyAudio Documentation](https://people.csail.mit.edu/hubert/pyaudio/docs/)\n")))}u.isMDXComponent=!0},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>f});var a=t(6540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach(function(n){o(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef(function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),m=o,f=u["".concat(l,".").concat(m)]||u[m]||d[m]||r;return t?a.createElement(f,i(i({ref:n},p),{},{components:t})):a.createElement(f,i({ref:n},p))});function f(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,i=new Array(r);i[0]=m;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"}}]);