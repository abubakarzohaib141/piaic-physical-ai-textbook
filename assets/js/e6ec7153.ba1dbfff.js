"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[5214],{3570:(e,n,a)=>{a.r(n),a.d(n,{contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=a(8168),r=(a(6540),a(5680));const o={},i="ROS 2 Nodes and Topics - Deep Dive",s={unversionedId:"physical-ai/module1-ros2/nodes-topics",id:"physical-ai/module1-ros2/nodes-topics",isDocsHomePage:!1,title:"ROS 2 Nodes and Topics - Deep Dive",description:"Understanding Nodes",source:"@site/docs/physical-ai/module1-ros2/nodes-topics.md",sourceDirName:"physical-ai/module1-ros2",slug:"/physical-ai/module1-ros2/nodes-topics",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module1-ros2/nodes-topics",editUrl:"https://github.com/abubakarzohaib141/piaic-physical-ai-textbook/edit/main/docs/docs/physical-ai/module1-ros2/nodes-topics.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Introduction to Physical AI & Humanoid Robotics",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/introduction"},next:{title:"Module 1: The Robotic Nervous System (ROS 2)",permalink:"/piaic-physical-ai-textbook/docs/physical-ai/module1-ros2/overview"}},l=[{value:"Understanding Nodes",id:"understanding-nodes",children:[{value:"The Lifecycle of a Node",id:"the-lifecycle-of-a-node",children:[]},{value:"Creating a Lifecycle Node",id:"creating-a-lifecycle-node",children:[]}]},{value:"Topics: The Data Highway",id:"topics-the-data-highway",children:[{value:"Common Message Types",id:"common-message-types",children:[]},{value:"Creating Custom Messages",id:"creating-custom-messages",children:[]}]},{value:"Advanced Topic Patterns",id:"advanced-topic-patterns",children:[{value:"Time Synchronization",id:"time-synchronization",children:[]},{value:"Transform Broadcasting (tf2)",id:"transform-broadcasting-tf2",children:[]}]},{value:"Topic Performance Optimization",id:"topic-performance-optimization",children:[{value:"Choosing the Right QoS",id:"choosing-the-right-qos",children:[]},{value:"Message Throttling",id:"message-throttling",children:[]}]},{value:"Practical Example: Humanoid Vision System",id:"practical-example-humanoid-vision-system",children:[]},{value:"Debugging Topics",id:"debugging-topics",children:[{value:"Command-Line Tools",id:"command-line-tools",children:[]},{value:"Using rqt_graph",id:"using-rqt_graph",children:[]}]},{value:"Best Practices",id:"best-practices",children:[]},{value:"Next Steps",id:"next-steps",children:[]},{value:"Resources",id:"resources",children:[]}],c={toc:l},m="wrapper";function g({components:e,...n}){return(0,r.yg)(m,(0,t.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"ros-2-nodes-and-topics---deep-dive"},"ROS 2 Nodes and Topics - Deep Dive"),(0,r.yg)("h2",{id:"understanding-nodes"},"Understanding Nodes"),(0,r.yg)("p",null,"In ROS 2, ",(0,r.yg)("strong",{parentName:"p"},"nodes are the atomic units of computation"),". Each node is a process that performs a specific task in your robot system."),(0,r.yg)("h3",{id:"the-lifecycle-of-a-node"},"The Lifecycle of a Node"),(0,r.yg)("p",null,"ROS 2 introduces ",(0,r.yg)("strong",{parentName:"p"},"managed nodes")," with explicit lifecycle states:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-mermaid"},"stateDiagram-v2\n    [*] --\x3e Unconfigured\n    Unconfigured --\x3e Inactive: configure()\n    Inactive --\x3e Active: activate()\n    Active --\x3e Inactive: deactivate()\n    Inactive --\x3e Unconfigured: cleanup()\n    Unconfigured --\x3e [*]: shutdown()\n    Active --\x3e [*]: shutdown()\n")),(0,r.yg)("p",null,"This allows for controlled startup and shutdown sequences, crucial for real robots."),(0,r.yg)("h3",{id:"creating-a-lifecycle-node"},"Creating a Lifecycle Node"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from rclpy.lifecycle import Node as LifecycleNode\nfrom rclpy.lifecycle import TransitionCallbackReturn, State\n\nclass MyLifecycleNode(LifecycleNode):\n    def __init__(self, node_name):\n        super().__init__(node_name)\n        \n    def on_configure(self, state: State) -> TransitionCallbackReturn:\n        self.get_logger().info('Configuring...')\n        # Initialize resources (open files, connect to hardware)\n        return TransitionCallbackReturn.SUCCESS\n        \n    def on_activate(self, state: State) -> TransitionCallbackReturn:\n        self.get_logger().info('Activating...')\n        # Start publishing/processing\n        return super().on_activate(state)\n        \n    def on_deactivate(self, state: State) -> TransitionCallbackReturn:\n        self.get_logger().info('Deactivating...')\n        # Stop publishing/processing\n        return super().on_deactivate(state)\n        \n    def on_cleanup(self, state: State) -> TransitionCallbackReturn:\n        self.get_logger().info('Cleaning up...')\n        # Release resources\n        return TransitionCallbackReturn.SUCCESS\n")),(0,r.yg)("h2",{id:"topics-the-data-highway"},"Topics: The Data Highway"),(0,r.yg)("p",null,"Topics implement the ",(0,r.yg)("strong",{parentName:"p"},"Publisher-Subscriber")," pattern, enabling:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Decoupling"),": Publishers don't know who subscribes"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Many-to-Many"),": Multiple publishers and subscribers per topic"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Asynchronous"),": No blocking waits")),(0,r.yg)("h3",{id:"common-message-types"},"Common Message Types"),(0,r.yg)("h4",{id:"geometry-messages"},"Geometry Messages"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from geometry_msgs.msg import Twist, Pose, PoseStamped, Transform\n\n# Velocity commands (for mobile robots)\ncmd_vel = Twist()\ncmd_vel.linear.x = 0.5   # m/s forward\ncmd_vel.angular.z = 0.3  # rad/s turn\n\n# Position and orientation\npose = Pose()\npose.position.x = 1.0\npose.position.y = 2.0\npose.position.z = 0.0\npose.orientation.w = 1.0  # Quaternion (no rotation)\n")),(0,r.yg)("h4",{id:"sensor-messages"},"Sensor Messages"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'from sensor_msgs.msg import Image, LaserScan, Imu, JointState\n\n# Camera image\nimage = Image()\nimage.height = 480\nimage.width = 640\nimage.encoding = "rgb8"\nimage.data = camera_data\n\n# LiDAR scan\nscan = LaserScan()\nscan.angle_min = -3.14\nscan.angle_max = 3.14\nscan.range_min = 0.1\nscan.range_max = 30.0\nscan.ranges = distance_measurements\n\n# IMU (Inertial Measurement Unit)\nimu = Imu()\nimu.linear_acceleration.x = accel_x\nimu.angular_velocity.z = gyro_z\n')),(0,r.yg)("h4",{id:"standard-messages"},"Standard Messages"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'from std_msgs.msg import String, Int32, Float64, Bool, Header\n\n# Simple data types\nmsg = String()\nmsg.data = "Hello, ROS 2!"\n\n# Header (timestamp + frame info)\nheader = Header()\nheader.stamp = self.get_clock().now().to_msg()\nheader.frame_id = "base_link"\n')),(0,r.yg)("h3",{id:"creating-custom-messages"},"Creating Custom Messages"),(0,r.yg)("p",null,"For humanoid robots, you might need custom message types:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Create message definition")," (",(0,r.yg)("inlineCode",{parentName:"li"},"my_robot_msgs/msg/JointCommand.msg"),"):")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"# JointCommand.msg\nstring joint_name\nfloat64 position\nfloat64 velocity\nfloat64 effort\n")),(0,r.yg)("ol",{start:2},(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Update ",(0,r.yg)("inlineCode",{parentName:"strong"},"CMakeLists.txt"))," (for C++) or ",(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"strong"},"package.xml")),":")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-xml"},"<build_depend>rosidl_default_generators</build_depend>\n<exec_depend>rosidl_default_runtime</exec_depend>\n<member_of_group>rosidl_interface_packages</member_of_group>\n")),(0,r.yg)("ol",{start:3},(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Use in Python"),":")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'from my_robot_msgs.msg import JointCommand\n\ncmd = JointCommand()\ncmd.joint_name = "left_elbow"\ncmd.position = 1.57  # 90 degrees in radians\n')),(0,r.yg)("h2",{id:"advanced-topic-patterns"},"Advanced Topic Patterns"),(0,r.yg)("h3",{id:"time-synchronization"},"Time Synchronization"),(0,r.yg)("p",null,"For sensor fusion, you need synchronized messages:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from message_filters import Subscriber, ApproximateTimeSynchronizer\nfrom sensor_msgs.msg import Image, CameraInfo\n\nclass SensorFusion(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n        \n        # Create subscribers with message_filters\n        image_sub = Subscriber(self, Image, '/camera/image')\n        depth_sub = Subscriber(self, Image, '/camera/depth')\n        \n        # Synchronize with approximate time matching\n        self.sync = ApproximateTimeSynchronizer(\n            [image_sub, depth_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        self.sync.registerCallback(self.sync_callback)\n        \n    def sync_callback(self, image_msg, depth_msg):\n        # Process synchronized RGB + Depth\n        self.get_logger().info('Received synchronized frames')\n")),(0,r.yg)("h3",{id:"transform-broadcasting-tf2"},"Transform Broadcasting (tf2)"),(0,r.yg)("p",null,"For humanoid robots, tracking coordinate frames is essential:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from tf2_ros import TransformBroadcaster, TransformListener, Buffer\nfrom geometry_msgs.msg import TransformStamped\n\nclass TransformPublisher(Node):\n    def __init__(self):\n        super().__init__('transform_publisher')\n        self.br = TransformBroadcaster(self)\n        \n    def publish_hand_transform(self):\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'wrist'\n        t.child_frame_id = 'hand'\n        \n        # Position\n        t.transform.translation.x = 0.15\n        t.transform.translation.y = 0.0\n        t.transform.translation.z = 0.0\n        \n        # Orientation (quaternion)\n        t.transform.rotation.w = 1.0\n        \n        self.br.sendTransform(t)\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"class TransformListener(Node):\n    def __init__(self):\n        super().__init__('transform_listener')\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n        \n    def get_hand_in_world(self):\n        try:\n            # Look up transform from world to hand\n            trans = self.tf_buffer.lookup_transform(\n                'world',\n                'hand',\n                rclpy.time.Time()\n            )\n            return trans\n        except Exception as e:\n            self.get_logger().error(f'Transform lookup failed: {e}')\n")),(0,r.yg)("h2",{id:"topic-performance-optimization"},"Topic Performance Optimization"),(0,r.yg)("h3",{id:"choosing-the-right-qos"},"Choosing the Right QoS"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from rclpy.qos import (\n    QoSProfile,\n    ReliabilityPolicy,\n    HistoryPolicy,\n    DurabilityPolicy,\n    LivelinessPolicy\n)\n\n# High-frequency sensor data (LiDAR, camera)\nsensor_qos = QoSProfile(\n    reliability=ReliabilityPolicy.BEST_EFFORT,  # Drop if network congested\n    history=HistoryPolicy.KEEP_LAST,\n    depth=5,\n    durability=DurabilityPolicy.VOLATILE\n)\n\n# Critical commands (must arrive)\ncommand_qos = QoSProfile(\n    reliability=ReliabilityPolicy.RELIABLE,  # Resend until acknowledged\n    history=HistoryPolicy.KEEP_LAST,\n    depth=10,\n    durability=DurabilityPolicy.TRANSIENT_LOCAL\n)\n\n# Configuration (late joiners should receive)\nconfig_qos = QoSProfile(\n    reliability=ReliabilityPolicy.RELIABLE,\n    history=HistoryPolicy.KEEP_LAST,\n    depth=1,\n    durability=DurabilityPolicy.TRANSIENT_LOCAL  # Store last message\n)\n")),(0,r.yg)("h3",{id:"message-throttling"},"Message Throttling"),(0,r.yg)("p",null,"Reduce message rate for bandwidth-limited scenarios:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"class ThrottledSubscriber(Node):\n    def __init__(self):\n        super().__init__('throttled_subscriber')\n        self.last_process_time = self.get_clock().now()\n        self.throttle_period = 0.1  # Process max 10 Hz\n        \n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image',\n            self.image_callback,\n            10\n        )\n        \n    def image_callback(self, msg):\n        current_time = self.get_clock().now()\n        elapsed = (current_time - self.last_process_time).nanoseconds / 1e9\n        \n        if elapsed >= self.throttle_period:\n            # Process the image\n            self.process_image(msg)\n            self.last_process_time = current_time\n        # else: drop the message\n")),(0,r.yg)("h2",{id:"practical-example-humanoid-vision-system"},"Practical Example: Humanoid Vision System"),(0,r.yg)("p",null,"Here's a complete example of a vision processing node for humanoids:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass HumanoidVision(Node):\n    def __init__(self):\n        super().__init__('humanoid_vision')\n        \n        # CV Bridge for ROS <-> OpenCV conversion\n        self.bridge = CvBridge()\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/head_camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/head_camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n        \n        # Publishers\n        self.detected_objects_pub = self.create_publisher(\n            PoseArray,\n            '/vision/detected_objects',\n            10\n        )\n        \n        self.debug_image_pub = self.create_publisher(\n            Image,\n            '/vision/debug_image',\n            10\n        )\n        \n        self.camera_matrix = None\n        \n    def camera_info_callback(self, msg):\n        # Store camera intrinsics\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        \n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        \n        # Run object detection (simplified)\n        detected_objects = self.detect_objects(cv_image)\n        \n        # Publish results\n        pose_array = self.create_pose_array(detected_objects)\n        self.detected_objects_pub.publish(pose_array)\n        \n        # Publish debug visualization\n        debug_image = self.draw_detections(cv_image, detected_objects)\n        debug_msg = self.bridge.cv2_to_imgmsg(debug_image, encoding='bgr8')\n        debug_msg.header = msg.header\n        self.debug_image_pub.publish(debug_msg)\n        \n    def detect_objects(self, image):\n        # Placeholder for actual detection (YOLO, etc.)\n        # Returns list of bounding boxes\n        return []\n        \n    def create_pose_array(self, detections):\n        pose_array = PoseArray()\n        pose_array.header.stamp = self.get_clock().now().to_msg()\n        pose_array.header.frame_id = 'head_camera'\n        # Add poses for detected objects\n        return pose_array\n        \n    def draw_detections(self, image, detections):\n        # Draw bounding boxes on image\n        return image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidVision()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n")),(0,r.yg)("h2",{id:"debugging-topics"},"Debugging Topics"),(0,r.yg)("h3",{id:"command-line-tools"},"Command-Line Tools"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'# List all topics\nros2 topic list\n\n# Show topic information (type, publishers, subscribers)\nros2 topic info /cmd_vel\n\n# Echo messages in real-time\nros2 topic echo /camera/image\n\n# Show message rate\nros2 topic hz /camera/image\n\n# Publish a message manually\nros2 topic pub /cmd_vel geometry_msgs/msg/Twist "{linear: {x: 0.5}, angular: {z: 0.0}}"\n\n# Show topic bandwidth\nros2 topic bw /camera/image\n')),(0,r.yg)("h3",{id:"using-rqt_graph"},"Using rqt_graph"),(0,r.yg)("p",null,"Visualize node and topic connections:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"rqt_graph\n")),(0,r.yg)("p",null,"This shows a live graph of:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Active nodes (ovals)"),(0,r.yg)("li",{parentName:"ul"},"Topics (rectangles)"),(0,r.yg)("li",{parentName:"ul"},"Connections between them")),(0,r.yg)("h2",{id:"best-practices"},"Best Practices"),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"[!TIP]","\n",(0,r.yg)("strong",{parentName:"p"},"Use Descriptive Topic Names"),": Follow the pattern ",(0,r.yg)("inlineCode",{parentName:"p"},"/robot_name/sensor_type/data_type"),"\nExample: ",(0,r.yg)("inlineCode",{parentName:"p"},"/humanoid/head_camera/image_raw"))),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"[!WARNING]","\n",(0,r.yg)("strong",{parentName:"p"},"Avoid Large Messages at High Frequency"),": Sending 4K images at 60Hz will overwhelm most networks. Compress or throttle as needed.")),(0,r.yg)("blockquote",null,(0,r.yg)("p",{parentName:"blockquote"},"[!IMPORTANT]","\n",(0,r.yg)("strong",{parentName:"p"},"Always Add Timestamps"),": Include proper timestamps in message headers for time synchronization and logging.")),(0,r.yg)("h2",{id:"next-steps"},"Next Steps"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("a",{parentName:"strong",href:"/piaic-physical-ai-textbook/docs/physical-ai/module1-ros2/urdf"},"URDF Robot Description"))," - Learn how to describe your humanoid robot's structure"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},(0,r.yg)("a",{parentName:"strong",href:"/piaic-physical-ai-textbook/docs/physical-ai/module2-gazebo/overview"},"Module 2: Gazebo"))," - Simulate your robot in a physics engine")),(0,r.yg)("h2",{id:"resources"},"Resources"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://docs.ros.org/en/humble/Concepts/About-Nodes.html"},"ROS 2 Node Documentation")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://github.com/ros2/common_interfaces"},"Common Interfaces")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://docs.ros.org/en/humble/Concepts/About-Quality-of-Service-Settings.html"},"Quality of Service")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Tf2-Main.html"},"tf2 Tutorials"))))}g.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>m,yg:()=>u});var t=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function s(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=t.createContext({}),c=function(e){var n=t.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},m=function(e){var n=c(e.components);return t.createElement(l.Provider,{value:n},e.children)},g="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},p=t.forwardRef(function(e,n){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),g=c(a),p=r,u=g["".concat(l,".").concat(p)]||g[p]||d[p]||o;return a?t.createElement(u,i(i({ref:n},m),{},{components:a})):t.createElement(u,i({ref:n},m))});function u(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=p;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[g]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}p.displayName="MDXCreateElement"}}]);